{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在这里测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.snippets import to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_BERT_model(config_path='BERT/chinese_rbt6_L-6_H-768_A-12/bert_config_rbt6.json',\n",
    "                    checkpoint_path = 'BERT/chinese_rbt6_L-6_H-768_A-12/bert_model.ckpt',\n",
    "                    dict_path = 'BERT/chinese_rbt6_L-6_H-768_A-12/vocab.txt'):\n",
    "    \"\"\" load BERT \"\"\"\n",
    "    tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器\n",
    "    model = build_transformer_model(config_path, checkpoint_path)  # 建立模型，加载权重\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "\n",
    "tokenizer, model = load_BERT_model()\n",
    "\n",
    "\n",
    "# 编码测试\n",
    "token_ids, segment_ids = tokenizer.encode(u'')\n",
    "token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "\n",
    "print(token_ids, segment_ids)\n",
    "\n",
    "print('\\n ===== predicting =====\\n')\n",
    "print(model.predict([token_ids, segment_ids]))\n",
    "\n",
    "# print('\\n ===== reloading and predicting =====\\n')\n",
    "# model.save('test.model')\n",
    "# del model\n",
    "# model = keras.models.load_model('test.model')\n",
    "# print(type(model))\n",
    "# print(model.predict([token_ids, segment_ids]).shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_encoder(num_of_LSTMs=2):\n",
    "    \"\"\" simple LSTM encoder\n",
    "    \n",
    "        sentence sequence |-> embedding of sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=[None, 768])\n",
    "    sequence = inputs\n",
    "    for _ in range(num_of_LSTMs):\n",
    "        sequence, hidden_state, cell_state = layers.LSTM(768,return_state=True, return_sequences=True)(sequence)\n",
    "    outputs = layers.concatenate([hidden_state, cell_state], axis=1)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "sent_encoder = build_sentence_encoder()\n",
    "x = tf.random.normal([100, 10, 768])\n",
    "sent_encoder(x).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = sent_encoder.get_layer('lstm_41')\n",
    "for weight in lstm.weights:\n",
    "    print(weight.name, weight.shape)\n",
    "\n",
    "lstm.set_weights([tf.random.normal([768, 3072]), tf.random.normal([768, 3072]), tf.random.normal([3072])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(sent1, sent2):\n",
    "    \"\"\" similarity between two sentences\n",
    "    \n",
    "    naive choice is cosine similarity. could be replaced.\n",
    "    \"\"\"\n",
    "    return np.dot(sent1, sent2)\n",
    "\n",
    "\n",
    "def coverage_score(sentences, selection, alpha):\n",
    "    \"\"\" the coverage score of a subset of sentences \n",
    "    \n",
    "        it measure how much the selection covers all the information of the whole corpus.\n",
    "    \"\"\"\n",
    "    N, S = len(sentences), len(selection)\n",
    "    score = 0\n",
    "    for i in range(N):\n",
    "        score += tf.minimum(sum([sim(sentences[i], selection[j]) for j in range(S)]),\n",
    "                            alpha * sum([sim(sentences[i], sentences[j]) for j in range(N)])\n",
    "                            )\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def document_extract(sentence_embeddings, sentence_lengths, max_budget, alpha=0.9, paragraph_partitions=None):\n",
    "    \"\"\" extract key sentences from the whole corpus\n",
    "\n",
    "    sentence_embeddings: dtype is numpy.ndarray ; dimension = [len_of_sequence, embedding_dim]\n",
    "    sentence_lengths: a list of integers, each entry is the length of the corresponding sentence\n",
    "    max_budget: the maximal length of the extracted text\n",
    "    \n",
    "    \"\"\"\n",
    "    N = len(sentence_embeddings)\n",
    "    indices = set(range(N))\n",
    "    selection = set()\n",
    "    selection_length = 0\n",
    "    while True:\n",
    "        no_fit = True\n",
    "        print(indices - selection)\n",
    "        for i in indices - selection:\n",
    "            if selection_length + sentence_lengths[i] > max_budget:\n",
    "                continue\n",
    "\n",
    "            selection_indices = list(selection.union(set([i])))\n",
    "            if no_fit:    \n",
    "                best_increment = coverage_score(sentence_embeddings, sentence_embeddings[selection_indices], alpha)\n",
    "                best_id = i\n",
    "            else:\n",
    "                increment = coverage_score(sentence_embeddings, sentence_embeddings[selection_indices], alpha)\n",
    "                if increment > best_increment:\n",
    "                    best_increment = increment\n",
    "                    best_id = i\n",
    "            \n",
    "            no_fit = False\n",
    "\n",
    "        selection.add(best_id)\n",
    "        selection_length += sentence_lengths[best_id]\n",
    "\n",
    "        if no_fit:\n",
    "            break\n",
    "\n",
    "    return selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "{1, 2, 3, 4, 5, 6, 8, 9}\n",
      "{1, 2, 3, 4, 5, 6, 8}\n",
      "{1, 2, 3, 4, 5, 8}\n",
      "{1, 2, 3, 5, 8}\n",
      "{8, 1, 2, 3}\n",
      "{0, 4, 5, 6, 7, 9}\n"
     ]
    }
   ],
   "source": [
    "sentences = np.random.randn(10, 768)\n",
    "print(document_extract(sentences, [1] * 10, 6, alpha=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set()\n",
    "s.union(set([1]))\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae31a227306e8bec534bb5a64e4e73701d019b1363d9e1675bd99b73ac8c6ab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
